---
title: "HW_CH4_Rethinking"
author: "Rachel Kaufman"
date: "2022-09-29"
output:
  html_document: default
  pdf_document: default
---

**Homework Assignment: Statistical Rethinking Chapter 4**



```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

```{r}
library(tidyverse)
library(rstan)
library(tidybayes)
library(rethinking)
library(tidybayes.rethinking)
library(devtools)
library(patchwork)
library(bayesplot)
library(devtools)
```

HW Question 4E1
Easy Problems
**4E1**
In the model provided in the text for this question, line 1 containing mu. This is because mu represents our regression line which cointains the liklihood. 

**4E2**
There are two parameters in the model given above. These parameters are mu and sigma. 

**4E3**
$$
\text{Pr}(\mu, \sigma \mid y_i) = \frac{\Pi_i\text{Normal}(y_i \mid \mu, \sigma)\text{Normal}(\mu \mid 0, 10)\text{Exponential}(\sigma \mid 1)}{\iint \Pi_i\text{Normal}(y_i \mid \mu, \sigma)\text{Normal}(\mu \mid 0, 10)\text{Exponential}(\sigma \mid 1)d\mu d\sigma}
$$
**4E4**
The linear model provided in the text can be seen in line 2.

**4E5**
There are three parameters here, sigma, alpha, and beta. With this we know  
that mu is no longer a parameter in our posterior because alpha and beta define this!

Medium Problems
**4M1**

```{r}
n <- 1e4
set.seed(4)

sim_model <- 
  tibble(
      mu_prior = rnorm(n, 0, 10), 
      sigma_prior = rexp(n, 1) 
      ) %>% 
      mutate(y_prior = rnorm(n, mu_prior, sigma_prior))

qplot(y_prior, data = sim_model)
```

**4M2**
```{r}
quap_formula <- alist(
    y ~ dnorm( mu, sigma),
    mu_prior ~ rnorm(n, 0, 10), 
    sigma_prior ~ rexp(n, 1)
)
```

**4M3**
$$
\begin{aligned}
y_i &\sim Normal(\mu, \sigma)\\
\mu_i &= \alpha + \beta(x_i) \\
\alpha &\sim Normal(0,10) \\
\beta &\sim Uniform(0,1) \\
\sigma &\sim Exponential(1) \\
\end{aligned}
$$
**4M4**
##if year is the predictor that means it is my x
$$
\begin{aligned}
height_i &\sim Normal(\mu, \sigma)\\
\mu_i &= \alpha + \beta(year_i) \\
\alpha &\sim Normal(0,100) \\
\beta &\sim Uniform(0,10) \\
\sigma &\sim Uniform(0,15) \\
\end{aligned}
$$

**4M5**
Yes, I would change my priors! This is what they would now look like. I started  
to consider how beta then should not have any negatives, so I then re-changed my  
priors to log my beta. 
$$
\begin{aligned}
height_i &\sim Normal(\mu, \sigma)\\
\mu_i &= \alpha + \beta(year_i) \\
\alpha &\sim Normal(0,100) \\
\beta &\sim LogNormal(0,1) \\
\sigma &\sim Uniform(0,15) \\
\end{aligned}
$$

**4M6**
Knowing this variance of 64cm, I am considering changing my priors for sigma, however; I am concerned with if this height variance is for the sample or the  
population. If it is the sample, this would be over-fitting to the data. Right? I would not change my prior, still. I could change it if my logic is wrong here,   
and square root it (because the square root of a variance is my SD) to 8. 

**4M7**
Loading the data, creating my condition for over the age of 18,  
and creating a new variable for the mean centered weight. 
```{r}
#loading the data
data(Howell1) 
d <- Howell1 
d2 <- d %>%
  filter(age >= 18) #d2 for my new condition of age being 18+

d2 <- d2 %>%
  mutate(
    cweight = weight - mean(weight))  ##mean center my bestie weight!
```

fitting the model for un-centered (default) weight:
```{r}
m4.3_uncentered <- quap( 
    alist(
        height ~ dnorm(mu , sigma),
        mu <- a + b*(d2$weight), ##this is where it is no longer centered
        a ~ dnorm( 178 , 20 ),
        b ~ dlnorm( 0 , 1 ),
        sigma ~ dunif( 0 , 50 )),
    data = d2) 
```

fitting the model for centered mean for weight:
```{r}
m4.3_cweight <- quap( 
  alist(
      height ~ dnorm(mu, sigma),
      mu <- a + b*(cweight), ##i replaced this weight-xbar
      a ~ dnorm( 178 , 20),
      b ~ dlnorm( 0 , 1),
      sigma ~ dunif( 0 , 50)), 
  data = d2)
```


Let's look at some of the numbers just to see what is happening.
```{r}
precis(m4.3_cweight) 
precis(m4.3_uncentered)
```

Now that I have set up both the centered weight and un-centered weight as two distinct models, on to the next steps. It would be helpful to have actual plots to be able to compare my models and look at the covariance. :)

UNCENTERED/Reg mean plotting:
```{r}
# take our samples for un-centered first
draws_uncentered <- tidy_draws(m4.3_uncentered, n = 100)
head(draws_uncentered)

# creating the plot for un-centered 
uncentered_plot <- ggplot(draws_uncentered) +
  geom_abline(aes(intercept = a, slope = b),
       alpha = .2) +
  geom_point(data = d2, mapping = aes(
             x = weight,
             y = height),
             alpha = .2) +
         labs(
            x = "weight in kg",
            y = "height in cm",
            title = "Posterior estimates")

# adding sigma to the plot to show the variance around lines :)
uncentered_plot_sims <- predicted_draws(m4.3_uncentered,
  newdata = d2,
  draws = 1000)
head(uncentered_plot_sims, 20)

uncentered_plot_sims <- uncentered_plot_sims %>%
  group_by(.row) %>%
  mutate(
    lo_bound = HPDI(.prediction)[1],
    up_bound = HPDI(.prediction)[2])

##final steps for first plot (I freaking hope)
uncentered_plot <- uncentered_plot + geom_ribbon(data = uncentered_plot_sims,
    mapping = aes(x = weight, ymax = up_bound, ymin = lo_bound),
  alpha = .1) +
  labs(caption = "89% HPDI overlaid")
```


Now, my SECOND plot where the mean of weight is CENTERED. See line 139 for QUAP()  
```{r}
# take our samples again
draws_cweight <- tidy_draws(m4.3_cweight, n = 100)
head(draws_cweight)

# actually create plot
cweight_plot <- ggplot(draws_cweight) +
  geom_abline(aes(intercept = a, slope = b),
  alpha = .2) +
  geom_point(data = d2, mapping = aes(x = cweight,y = height),
  alpha = .2) +
  labs(
    x = "weight - mean(weight) in kg",
    y = "height in cm",
    title = "Posterior estimates",
    subtitle = "mean centered weight")
  
# now let's work on adding my variance (sigma)
cweight_sims <- predicted_draws(m4.3_cweight,
  newdata = d2,
  draws = 1000)
head(cweight_sims, 20)

cweight_sims <- cweight_sims %>%
  group_by(.row) %>%
  mutate(
    lo_bound = HPDI(.prediction)[1],
    up_bound = HPDI(.prediction)[2])

# Something about this chunk of code has an incredibly weird result  
#of my sigma being completely off
cweight_plot <- cweight_plot +
  geom_ribbon(data = cweight_sims,
  mapping = aes(x = cweight, ymax = up_bound, ymin = lo_bound),
  alpha = .1) +
  labs(caption = "89% HPDI overlaid")
```

plot them together side by side!
```{r}
uncentered_plot + cweight_plot
```


Overall, I would assume that the covariance matrix would remain similar but with the covariance as you shift the value your covariance is less. My magnitude seems to be greater but the general meaning between the two stays the same


Doing it with numbers:
```{r}
vcov(m4.3_cweight)
vcov(m4.3_uncentered)
```
In terms of the covariance just looking and the output from the vcov() argument,the covariance is a lot neater and tighter for the centered mean in comparison to the use of the regular mean. 


**4M8**
```{r}
library(splines)
# load in data
data(cherry_blossoms)
d <- cherry_blossoms
precis(d)

# create new data frame
d2 <- d[complete.cases(d$doy), ]
num_knots <- 15
num_knots_c <- 30
knot_list <- quantile(d2$year, probs = seq(0, 1, length.out = num_knots))
knot_list_c <- quantile(d2$year, probs = seq(0, 1, length.out = num_knots_c))

B <- bs(d2$year,
  knots = knot_list[-c(1, num_knots)],
  degree = 3, intercept = TRUE)

C <- bs(d2$year,
  knots = knot_list_c[-c(1, num_knots_c)],
  degree = 3, intercept = TRUE)

# plot the two
plot(NULL, xlim = range(d2$year), ylim = c(0, 1), 
     xlab = "year", ylab = "basis")
for (i in 1:ncol(B)) lines(d2$year, B[, i])

plot(NULL, xlim = range(d2$year), ylim = c(0, 1), 
     xlab = "year", ylab = "basis")
for (i in 1:ncol(C)) lines(d2$year, C[, i]) 

m4.7 <- quap( 
      alist(
      D ~ dnorm( mu , sigma),
      mu <- a + B %*% w ,
      a ~ dnorm(100,10),
      w ~ dnorm(0,10),
      sigma ~ dexp(3)
      ), data = list(D = d2$doy, B = B),
start = list( w = rep( 0 , ncol(B))))

post <- extract.samples( m4.7 ) 
w <- apply( post$w, 2, mean)
plot( NULL , xlim = range(d2$year), ylim = c(-6,6),
  xlab = "year", ylab = "basis * weight")
  for (i in 1:ncol(B) ) lines(d2$year, w[i]*B[,i])


```

Firstly, when you increase the number of knots from the original 15, the spline has a greater range on the y-axis, where the dips in the graph become more wiggly.Conversely, adjusting the priors, i.e. shrinking them, leads to a tighter range in the Y-axis, where our spline looks less wiggly. 









































